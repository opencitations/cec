======= Header metadata ======= 

Evaluation on 1942 random PDF files out of 1942 PDF (ratio 1.0).

======= Strict Matching ======= (exact matches)

===== Field-level results =====

label			accuracy	precision	recall		f1

title			94.73		69.59		66.58		68.05
authors			94.09		64.73		62.53		63.61
first_author		98.3		92.87		89.28		91.04
abstract		86.71		15.91		14.66		15.26
keywords		94.84		67.38		54.68		60.37

all fields		93.73		62.2		57.86 		59.95	(micro average)
			93.73		62.1		57.54		59.67	(macro average)


======== Soft Matching ======== (ignoring punctuation, case and space characters mismatches)

===== Field-level results =====

label			accuracy	precision	recall		f1

title			95.46		76.48		73.17		74.79
authors			93.92		66.33		64.07		65.18
first_author		98.28		93.46		89.85		91.62
abstract		90.71		48.3		44.5		46.32
keywords		95.25		75.51		61.28		67.65

all fields		94.73		71.99		66.97 		69.39	(micro average)
			94.73		72.02		66.57		69.11	(macro average)


==== Levenshtein Matching ===== (Minimum Levenshtein distance at 0.8)

===== Field-level results =====

label			accuracy	precision	recall		f1

title			96.35		84.12		80.48		82.26
authors			95.43		78.28		75.62		76.93
first_author		98.1		93.51		89.9		91.67
abstract		95.36		81.53		75.13		78.2
keywords		96.2		89.01		72.23		79.74

all fields		96.29		85		79.08 		81.94	(micro average)
			96.29		85.29		78.67		81.76	(macro average)


= Ratcliff/Obershelp Matching = (Minimum Ratcliff/Obershelp similarity at 0.95)

===== Field-level results =====

label			accuracy	precision	recall		f1

title			95.71		79.66		76.21		77.89
authors			94.17		70.06		67.68		68.85
first_author		98.06		92.87		89.28		91.04
abstract		94.67		76.25		70.26		73.13
keywords		95.83		84		68.17		75.26

all fields		95.69		80.31		74.71 		77.41	(micro average)
			95.69		80.57		74.32		77.24	(macro average)

===== Instance-level results =====

Total expected instances: 	1942
Total correct instances: 	141 (strict) 
Total correct instances: 	400 (soft) 
Total correct instances: 	803 (Levenshtein) 
Total correct instances: 	646 (ObservedRatcliffObershelp) 

Instance-level recall:	7.26	(strict) 
Instance-level recall:	20.6	(soft) 
Instance-level recall:	41.35	(Levenshtein) 
Instance-level recall:	33.26	(RatcliffObershelp) 

======= Citation metadata ======= 

Evaluation on 1942 random PDF files out of 1942 PDF (ratio 1.0).

======= Strict Matching ======= (exact matches)

===== Field-level results =====

label			accuracy	precision	recall		f1

title			97.36		76.39		68.95		72.48
authors			97.77		80.87		69.84		74.95
first_author		98.55		88.16		76.01		81.64
date			98.95		91.11		77.21		83.59
inTitle			96.59		69.82		66.4		68.07
volume			99.13		93.85		81.7		87.36
issue			99.43		80.92		78.24		79.56
page			98.7		90.96		78.6		84.33

all fields		98.31		84.14		74.22 		78.87	(micro average)
			98.31		84.01		74.62		79	(macro average)


======== Soft Matching ======== (ignoring punctuation, case and space characters mismatches)

===== Field-level results =====

label			accuracy	precision	recall		f1

title			98.53		87.61		79.09		83.13
authors			97.78		81.48		70.37		75.52
first_author		98.53		88.31		76.14		81.78
date			98.92		91.11		77.21		83.59
inTitle			97.71		80.25		76.32		78.24
volume			99.11		93.85		81.7		87.36
issue			99.42		80.92		78.24		79.56
page			98.67		90.96		78.6		84.33

all fields		98.58		87.33		77.03 		81.85	(micro average)
			98.58		86.81		77.21		81.69	(macro average)


==== Levenshtein Matching ===== (Minimum Levenshtein distance at 0.8)

===== Field-level results =====

label			accuracy	precision	recall		f1

title			98.79		90.14		81.37		85.53
authors			98.36		86.81		74.97		80.46
first_author		98.54		88.5		76.31		81.95
date			98.91		91.11		77.21		83.59
inTitle			97.81		81.33		77.34		79.29
volume			99.1		93.85		81.7		87.36
issue			99.41		80.92		78.24		79.56
page			98.65		90.96		78.6		84.33

all fields		98.7		88.61		78.16 		83.06	(micro average)
			98.7		87.95		78.22		82.76	(macro average)


= Ratcliff/Obershelp Matching = (Minimum Ratcliff/Obershelp similarity at 0.95)

===== Field-level results =====

label			accuracy	precision	recall		f1

title			98.7		89.21		80.53		84.65
authors			98.05		83.95		72.5		77.81
first_author		98.52		88.19		76.04		81.67
date			98.92		91.11		77.21		83.59
inTitle			97.54		78.88		75.02		76.9
volume			99.11		93.85		81.7		87.36
issue			99.42		80.92		78.24		79.56
page			98.66		90.96		78.6		84.33

all fields		98.61		87.68		77.34 		82.18	(micro average)
			98.61		87.14		77.48		81.98	(macro average)

===== Instance-level results =====

Total expected instances: 		90068
Total extracted instances: 		91790
Total correct instances: 		35456 (strict) 
Total correct instances: 		46338 (soft) 
Total correct instances: 		50296 (Levenshtein) 
Total correct instances: 		47290 (RatcliffObershelp) 

Instance-level precision:	38.63 (strict) 
Instance-level precision:	50.48 (soft) 
Instance-level precision:	54.79 (Levenshtein) 
Instance-level precision:	51.52 (RatcliffObershelp) 

Instance-level recall:	39.37	(strict) 
Instance-level recall:	51.45	(soft) 
Instance-level recall:	55.84	(Levenshtein) 
Instance-level recall:	52.5	(RatcliffObershelp) 

Instance-level f-score:	38.99 (strict) 
Instance-level f-score:	50.96 (soft) 
Instance-level f-score:	55.31 (Levenshtein) 
Instance-level f-score:	52.01 (RatcliffObershelp) 

Matching 1 :	62301

Matching 2 :	3434

Matching 3 :	2964

Matching 4 :	651

Total matches :	69350

======= Fulltext structures ======= 

Evaluation on 1942 random PDF files out of 1942 PDF (ratio 1.0).

======= Strict Matching ======= (exact matches)

===== Field-level results =====

label			accuracy	precision	recall		f1

section_title		94.1		69.92		61.31		65.33
reference_citation	58.77		54.24		41.61		47.09
reference_figure	93.11		46.65		58.3		51.83
reference_table		97.59		0		0		0
figure_title		97.15		35.62		28.21		31.49
figure_caption		90.36		0		0		0
table_title		98.76		0		0		0
figure_caption		98.44		0		0		0

all fields		91.03		54.83		36.86 		44.08	(micro average)
			91.03		25.8		23.68		24.47	(macro average)


======== Soft Matching ======== (ignoring punctuation, case and space characters mismatches)

===== Field-level results =====

label			accuracy	precision	recall		f1

section_title		94.57		73.6		64.53		68.77
reference_citation	60.7		58.57		44.93		50.85
reference_figure	93.02		47.18		58.95		52.41
reference_table		97.53		0		0		0
figure_title		98.58		75.47		59.77		66.71
figure_caption		90.62		0		0		0
table_title		98.72		0		0		0
figure_caption		98.43		0		0		0

all fields		91.52		59.74		40.16 		48.03	(micro average)
			91.52		31.85		28.52		29.84	(macro average)


************************************************************************************
COUNTER: org.grobid.core.engines.TaggingLabels
************************************************************************************
------------------------------------------------------------------------------------
  QUANTITY_VALUE_BASE:   0
  UNIT_VALUE_POW:        0
  UNIT_VALUE_PREFIX:     0
  PARAGRAPH:             134082
  FIG_LABEL:             5238
  TBL_TRASH:             7228
  FIGURE_MARKER:         18014
  TABLE:                 8190
  FIG_TRASH:             2441
  QUANTITY_VALUE_MOST:   0
  TBL_DESC:              686
  CITATION_MARKER:       71615
  EQUATION:              2864
  QUANTITY_OTHER:        0
  FIG_HEAD:              8975
  QUANTITY_VALUE_RANGE:  0
  TBL_HEAD:              9001
  UNIT_VALUE_OTHER:      0
  QUANTITY_VALUE_LEAST:  0
  OTHER:                 0
  ITEM:                  0
  QUANTITY_VALUE_ATOMIC: 0
  FIG_OTHER:             0
  TBL_LABEL:             4256
  TABLE_MARKER:          6437
  FIG_DESC:              7522
  TBL_OTHER:             0
  UNIT_VALUE_BASE:       0
  QUANTITY_UNIT_LEFT:    0
  QUANTITY_VALUE_LIST:   0
  FIGURE:                9825
  QUANTITY_UNIT_RIGHT:   0
  SECTION:               26331
====================================================================================

************************************************************************************
COUNTER: org.grobid.core.data.Table$TableRejectionCounters
************************************************************************************
------------------------------------------------------------------------------------
  CANNOT_PARSE_LABEL_TO_INT:          401
  CONTENT_SIZE_TOO_SMALL:             124
  CONTENT_WIDTH_TOO_SMALL:            13
  FEW_TOKENS_IN_CONTENT:              1
  EMPTY_LABEL_OR_HEADER_OR_CONTENT:   3929
  HEADER_NOT_STARTS_WITH_TABLE_WORD:  93
  HEADER_AREA_BIGGER_THAN_CONTENT:    238
  HEADER_AND_CONTENT_DIFFERENT_PAGES: 13
  HEADER_NOT_CONSECUTIVE:             437
  HEADER_AND_CONTENT_INTERSECT:       702
  FEW_TOKENS_IN_HEADER:               0
====================================================================================

************************************************************************************
COUNTER: org.grobid.core.utilities.matching.ReferenceMarkerMatcher$Counters
************************************************************************************
------------------------------------------------------------------------------------
  UNMATCHED_REF_MARKERS:                    10759
  MATCHED_REF_MARKERS_AFTER_POST_FILTERING: 2273
  STYLE_AUTHORS:                            30785
  STYLE_NUMBERED:                           36157
  MANY_CANDIDATES:                          3318
  MANY_CANDIDATES_AFTER_POST_FILTERING:     449
  NO_CANDIDATES:                            18083
  MATCHED_REF_MARKERS:                      84091
  INPUT_REF_STRINGS_CNT:                    71615
  NO_CANDIDATES_AFTER_POST_FILTERING:       558
  STYLE_OTHER:                              4673
====================================================================================
====================================================================================