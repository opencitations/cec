<?xml version="1.0" encoding="UTF-8"?>
<patent-document status="new" lang="EN" ucid="EP-1126436-B1" country="EP" doc-number="1126436" kind="B1" date="20060607"><bibliographic-data><publication-reference fvid="23218745" ucid="EP-1126436-B1" status="new"><document-id status="new" format="original"><country>EP</country><doc-number>1126436</doc-number><kind>B1</kind><date>20060607</date><lang>EN</lang></document-id></publication-reference><application-reference ucid="EP-01301381-A" status="new" is-representative="NO"><document-id format="epo" status="new"><country>EP</country><doc-number>01301381</doc-number><kind>A</kind><date>20010216</date><lang>EN</lang></document-id></application-reference><priority-claims status="new"><priority-claim ucid="GB-0003903-A" status="new"><document-id format="epo" status="new"><country>GB</country><doc-number>0003903</doc-number><kind>A</kind><date>20000218</date></document-id></priority-claim></priority-claims><dates-of-public-availability status="new"><intention-to-grant-date><date>20051117</date></intention-to-grant-date></dates-of-public-availability><term-of-grant /><technical-data status="new"><classifications-ipcr><classification-ipcr status="new">G06F   3/033       20060101ALI20010803BHEP        </classification-ipcr><classification-ipcr status="new">G06F   3/033       20060101CLI20010803BHEP        </classification-ipcr><classification-ipcr status="new">G06F   3/16        20060101AFI20060310RMJP        </classification-ipcr><classification-ipcr status="new">G06F   3/16        20060101ALI20010803BHEP        </classification-ipcr><classification-ipcr status="new">G06F   3/16        20060101CFI20060310RMJP        </classification-ipcr><classification-ipcr status="new">G06F  17/27        20060101ALI20010803BHEP        </classification-ipcr><classification-ipcr status="new">G06F  17/27        20060101CLI20010803BHEP        </classification-ipcr><classification-ipcr status="new">G10L  15/00        20060101CFI20010530BHEP        </classification-ipcr><classification-ipcr status="new">G10L  15/14        20060101ALI20060310RMJP        </classification-ipcr><classification-ipcr status="new">G10L  15/18        20060101ALI20010803BHEP        </classification-ipcr><classification-ipcr status="new">G10L  15/24        20060101AFI20010530BHEP        </classification-ipcr><classification-ipcr status="new">G10L  15/26        20060101ALI20010803BHEP        </classification-ipcr><classification-ipcr status="new">G10L  15/28        20060101A I20051008RMEP        </classification-ipcr></classifications-ipcr><classification-ecla status="new"><classification-symbol scheme="EC">G10L15/24</classification-symbol><classification-symbol scheme="EC">G10L15/28M</classification-symbol></classification-ecla><invention-title lang="DE" status="new">Spracherkennung aus multimodalen Eingabe</invention-title><invention-title lang="EN" status="new">Speech recognition from multimodal inputs</invention-title><invention-title lang="FR" status="new">Reconnaissance de parole à l'aide de données multi-modales</invention-title><citations /><figures /></technical-data><parties><applicants><applicant format="epo" status="new"><addressbook><name>CANON KK</name><address><country>JP</country></address></addressbook></applicant><applicant format="intermediate" status="new"><addressbook><name>CANON KABUSHIKI KAISHA</name><address><country /></address></addressbook></applicant></applicants><inventors><inventor format="epo" status="new"><addressbook><name>FORTESCUE NICOLAS DAVIS</name><address><country>GB</country></address></addressbook></inventor><inventor format="epo" status="new"><addressbook><name>KEILLER ROBERT ALEXANDER</name><address><country>GB</country></address></addressbook></inventor><inventor format="intermediate" status="new"><addressbook><name>FORTESCUE, NICOLAS DAVIS</name><address><country /></address></addressbook></inventor><inventor format="intermediate" status="new"><addressbook><name>KEILLER, ROBERT ALEXANDER</name><address><country /></address></addressbook></inventor><inventor format="original" status="new"><addressbook><last-name>FORTESCUE, NICOLAS DAVIS</last-name><address><street>2 Curling Vale</street><city>Guildford GU2 5PJ</city><country>GB</country></address></addressbook></inventor><inventor format="original" status="new"><addressbook><last-name>KEILLER, ROBERT ALEXANDER</last-name><address><street>47 Guildford Park Avenue</street><city>Guildford GU2 5NI</city><country>GB</country></address></addressbook></inventor></inventors><assignees><assignee format="original" status="new"><addressbook><last-name>CANON KABUSHIKI KAISHA</last-name><address><street>30-2, 3-chome, Shimomaruko, Ohta-ku</street><city>Tokyo</city><country>JP</country></address></addressbook></assignee></assignees><agents><agent format="original" status="new"><addressbook><last-name>Beresford, Keith Denis Lewis</last-name><address><street>BERESFORD &amp; Co. 16 High Holborn</street><city>London WC1V 6BX</city><country>GB</country></address></addressbook></agent></agents></parties><international-convention-data><designated-states><ep-contracting-states><country>DE</country><country>FR</country><country>GB</country></ep-contracting-states></designated-states></international-convention-data></bibliographic-data><abstract lang="EN" source="EPO" status="new"><p>A speech recognition method for use in a multimodal input system comprises receiving a multimodal input comprising digitized speech as a first modality input and data in at least one further modality input. Features in the speech and in the data in at least one further modality are identified. The identified features in the speech and in the data are used in the recognition of words by comparing the identified features with states in models for the words. The models have states for the recognition of speech and for words having features in at least one further modality associated with the words, the models also have states for the recognition of events in the or each further modality. &lt;IMAGE&gt;</p></abstract><abstract lang="EN" status="new"><p num="">A speech recognition method for use in a multimodal input
system comprises receiving a multimodal input comprising
digitized speech as a first modality input and data in at
least one further modality input. Features in the speech
and in the data in at least one further modality are
identified. The identified features in the speech and in
the data are used in the recognition of words by
comparing the identified features with states in models
for the words. The models have states for the
recognition of speech and for words having features in at
least one further modality associated with the words, the
models also have states for the recognition of events in
the or each further modality.
<img file="00000001.tif" id="img-EP-1126854-A1-00000001" he="102" wi="151" img-format="tif" img-content="ad" orientation="unknown" inline="no" /></p></abstract><description lang="EN" status="new"><p num="p0001">The present invention generally relates to the improvement of the accuracy of speech recognition in a complementary multimodal input system.</p><p num="p0002">Interfaces which use speech as an input and at least one further modality input are known as multimodal systems. In multimodal systems where two modalities contain the same information content they are termed redundant e.g. speech recognition and lip movement recognition. Where two modalities each contain their own information they are termed complementary e.g. speech recognition and eyebrow movement recognition (since although eyebrow movement can be related to speech, it can include its own information e.g. emotion), and speech recognition and pointing events such as mouse clicks. Complementary modality input systems provide a more natural and powerful method of communication than any single modality can alone. The further modalities can for example comprise pointing events from pointing devices such as a mouse, touch screen, joystick, tracker ball, or a track pad, a pen input in which handwriting is recognised, or gesture recognition. Thus in complementary multimodal systems, parallel multimodal inputs are received and processed in order to control a system such as a computer.</p><p num="p0003">It is known that speech recognition engines do not always perform correct recognition on the speech.</p><p num="p0004"><ref type="patent">WO 00/08547</ref> describes a multilevel user interface in which a reasoner is provided to handle ambiguities such as &quot;give me this of that&quot; and to deal with conflicting information arriving from different modalities. The reasoner has a fuzzy temporal reasoner, a query pre-processor, a constraint checker, a wizard for resolving ambiguities and a post-processor plus a knowledge base containing facts and rules. The fuzzy temporal reasoner receives input events from a number of different modalities consisting of a gaze input, a recognised speech input STAP which comprises speech recognition software which recognises words in speech input, a mouse input and a keyboard input. The fuzzy temporal reasoner time stamps each received event and acts to &quot;fuzzify&quot; the time relationship between inputs from different modalities so that inputs from different modalities can be determined to have a temporal relationship if, for example, one of them occurs just before, during or overlapping with the other. The primary input modalities are the keyboard and the words resulting from the speech recognition by the recognised speech input, the mouse and gaze systems provide supplementary information to the reasoner when ambiguities in the user input regarding an object reference need to be resolved. On receipt ot an input from a mouse or gaze system, the fuzzy temporal reasoner looks for an incomplete command consisting of a word or words recognised by the speech recogniser STAP or input via the keyboard (an incomplete command may already have been received and require a mouse/gaze input to resolve an ambiguity, or an incomplete command may just about to be received). Where ambiguity cannot be resolved or individual parts of the query do not satisfy constraints, then a constraint checking module replaces the ambiguous terms by reserved words such as &quot;this&quot; and &quot;that&quot;. The wizard acts to resolve any such ambiguities in a hierarchical manner giving the focus of attention (gaze) the highest priority and the dialogue system the lowest.</p><p num="p0005"><ref type="patent">US-A-5,748,974</ref> describes a multimodal natural language interface for cross-application tasks wherein the user input may be spoken, typed, handwritten, mouse controlled cursor, touch or any other modality. Speech input via a microphone is supplied to a speech recogniser and the output of the speech recogniser and non-speech input received by a screen manager are sent to a dispatcher which combines the inputs and directs the combined inputs to a natural language processor which directs the combined multimodal input to a parser/semantic interpreter based on the output of the natural language processor, the dispatcher invokes an application manager to determine which application should process the request.</p><p num="p0006"><ref type="patent">EP-A-0505621</ref> is specifically concerned with integrating speech and handwriting information and is based upon the premise that the situations in which recognition errors occur in speech data are situations where recognition errors should not occur in handwriting data and vice versa, so that integrating the speech and handwriting data should enable a reduction in recognition errors. In this technique, respective feature vectors are generated independently for speech and handwriting data input, normally providing a four dimensional handwriting feature vector and a 20 dimensional speech data vector. The two feature vectors are normalised to a common time base and then simply combined into a single feature vector which may be a synthetic concatenation of the four dimensional handwriting feature vector and the 20 dimensional speech feature vector <ref type="patent">US-A-5781179</ref> describes a multimodal information input method and apparatus wherein a command for an application program is generated based both on movement of a cursor on a display unit by a pointing device and speech produced in parallel to the operation of the pointing device.</p><p num="p0007">In accordance with a first aspect, the present invention provides a method of recognising speech as set out in claim 1.</p><p num="p0008">In a second aspect the present invention provides speech recognition apparatus as set out in claim 6.</p><p num="p0009">An embodiment of the present invention improves the accuracy of speech recognition using the pointing device modality input in a complementary multimodal system.</p><p num="p0010">In a method embodying the invention, the models for the words used in the recognition utilise not just the features of the speech modality input, but also the features of the pointing device modality input. This greatly improves the recognition accuracy, since more data is available from a different source of information to aid recognition. The recognition engine will not recognise words as words which should have pointing device modality inputs if those further pointing device modality inputs have not been received in association with the spoken words.</p><p num="p0011">This invention is applicable to a complementary multimodal input system in which the improved speech recognition technique is used for the input of recognised words, and inputs to a processing system are generated by processing the recognised words and pointing device event data from the pointing device modality input in accordance with multimodal grammar rules. Thus in this aspect of the present invention, a more accurate input is achieved to the processing system.</p><p num="p0012">In an embodiment, the models for the words use states of Hidden Markov models for speech and states of finite state machines for the pointing device modality input. The transitions between states in the model have probabilities associated with them thereby resulting in an accumulated probability during the recognition process. Thus in this embodiment, in accordance with conventional speech recognition processes, a word is recognised which has the highest accumulated probability at a final state in the word model.</p><p num="p0013">The present invention can be implemented in dedicated specifically designed hardware. However, more preferably the present invention is implemented using a general purpose computer controlled by software. Thus the present invention encompasses program code for controlling a processor to implement the technique. The present invention can thus be embodied as a carrier medium carrying the program code. Such a carrier medium can for example comprise a storage medium such as a floppy disk, CD ROM, hard disk drive, or programmable read only memory device, or the carrier medium can comprise a signal such as an electrical signal carried over a network such as the Internet.</p><p num="p0014">An embodiment of the present invention will now be described with reference to the accompanying drawings, in which:<ul list-style="bullet"><li>Figure 1 is a schematic diagram of a complementary multimodal system in accordance with the embodiment of the present invention;</li><li>Figure 2 is a schematic diagram of an implementation of the multimodal system in accordance with the embodiment of the present invention;</li><li>Figure 3 is a flow diagram of the method of initialisation in the embodiment of the present invention;</li><li>Figure 4 is a flow diagram of the method of recognition in the embodiment of the present invention;</li><li>Figure 5 is a diagram illustrating the word net used in the recognition process of the embodiment of the present invention;</li><li>Figures 6a and 6b are diagrams illustrating finite state machines for a pointing device modality input; and</li><li>Figure 7 is a diagram of an array of states in a word model using the recognition process of the embodiment of the present invention.</li></ul></p><p num="p0015">Figure 1 is a schematic diagram of the complementary multimodal system of the embodiment of the present invention. The system can be divided into three functional sections. In an initialisation section a net of words (referred to hereinafter as a &quot;word net&quot; of word models is generated for use in the recognition section. A recognition section performs recognition on a multimodal input comprising an audio input and a second, pointing device, modality input and generates recognised text for input to a multimodal application section together with the second modality input.</p><p num="p0016">The initialisation section is provided with a multimodal grammar store 1. The multimodal grammar store 1 stores grammar rules to be used in the recognition process. The grammar rules comprise context free grammar and are defined as a function of not just the speech modality input, but also the second modality input. For example, for an instruction to send a document by facsimile to a destination, a grammar rule can be provided to allow a user to say &quot;fax this to him&quot; whilst pointing to a displayed document when saying &quot;this&quot; and subsequently pointing to a displayed identified person when saying &quot;him&quot;.</p><p num="p0017">To allow for synonyms to be used, thus providing a more flexible input system, a grammar rule can be defined as follows:

&lt;fax rule&gt;=&lt;fax word&gt;[(&lt;this word&gt;&amp;!click)|(these &amp;!click(2+))]to(&lt;him word&gt;&amp;!click)
where &lt;fax word&gt; = fax| send

&lt;this word&gt; = this|that

&lt;him word&gt; = him|her</p><p num="p0018">Thus the grammar rule open &lt;fax rule&gt; is defined as a function of recognised words (fax, send, this, that, these, to, him, her) and mouse events (clicks). The grammar rule &lt;fax rule&gt; can be defined in terms of variables such as &lt;fax word&gt; to allow for a simpler definition of the grammar rules.</p><p num="p0019">In the arrangement of Figure 1, the grammar rules are input from the grammar store 1 to a word net generator 3. The word net generator 3 uses the grammar rules in order to generate a word net. The word net generated using the grammar rule given above is illustrated in Figure 5 and has nodes 0 to 6. It can be seen that between certain nodes of the word net i.e. between nodes 1 and 2, 1 and 3, and 5 and 6, simultaneous mouse events are required. In order to accommodate these simultaneous mouse events in the recognition process using the word net, a finite state machine generator 2 is provided to generate a finite state machine for the second modality input.

Figure 6a illustrates the states of a finite state machine for the event comprising 2 or more mouse clicks. Figure 6b illustrates the states for an event comprising a single mouse click. The wordnet generator 3 receives the generated states and incorporates these within the word net as will be described in more detail hereinafter.</p><p num="p0020">The generated word net is then made available to the FSM processor 6 to perform recognition within the recognition section.</p><p num="p0021">The recognition section comprises an audio input device 4 for inputting digitized speech. A feature extractor 5 is provided to receive the digitized speech to extract features of the speech as is well known in the art. A second pointing device, modality input device 7 is provided for the input of second modality events such as mouse click events. A further feature extractor 7a is provided to extract features from the events. In this particular embodiment the feature extractor 7a simply identifies individual mouse clicks in an event which may comprise 1, 2, or more mouse clicks. The extracted features of the speech input and of the second modality input are then input into the FSM processor 6 which uses the generated word net in order to perform recognition. A recognised text output device 8 is provided to output the words recognised by the FSM processor 6 to the multimodal application 9 in the multimodal application section.</p><p num="p0022">In the multimodal application section, a multimodal application processor 9 is provided which receives the recognised text and the second modality input from the second modality input device 7 in order to generate an input to the application. Also, the multimodal application processor 9 receives grammar rules from a multimodal grammar store la which stores the same grammar rules as in the multimodal grammar store 1 in the initialisation section.</p><p num="p0023">Thus the multimodal application processor 9 is able to generate an input to a process based on an accurately recognised text which has been recognised taking into consideration the second modality input.</p><p num="p0024">Figure 2 is a schematic diagram of a practical implementation of the embodiment of the present invention. This embodiment of the present invention is carried out by the implementation of computer code by a general purpose computer.</p><p num="p0025">The computer comprises an audio input device 10 for inputting digitized speech. A pointing device 11 is provided for inputting the second modality input which in this case comprise mouse events comprising one or more mouse clicks. The computer is also provided with a conventional keyboard 12 and display 13.</p><p num="p0026">The computer comprises a processor 14 for implementing program code modules stored in a program memory 15. The program code modules can be supplied to the program memory 15 by a suitable carrier medium 16 such as a floppy disk. The processor 14 implements the FSM generator 14a by loading and running the FSM generator code module from the program memory 15. The processor also implements the word net generator 14b by loading and running the word net generator code module from the program memory 15. The processor further implements the FSM processor 14c by loading and running the FSM processor code module from the program memory 15. The processor 14 also implements the feature extractor 14d by loading the feature extractor code module from the program memory 15. The processor 14 further implements the text output module 14e by loading and running a text output code module stored in the program memory 15. The processor 14 further implements the multimodal application 14f by loading and running the multimodal application code module stored in the program memory 15. The processor 14 is the central processing unit (CPU) of the general purpose computer and the program memory 15 comprises any memory capable of storing programs such as random access memory (RAM), read only memory (ROM), a hard disk drive, or CD ROM.</p><p num="p0027">The computer is also provided with working memory 17 to be used by the processor 14 during the operation of the processor. The working memory 17 can store the generated finite state machines, the grammar rules, the word net data, and the recognised text. The working memory 17 can comprise any conventional memory accessible by the processor 14 such as random access memory (RAM).</p><p num="p0028">The processor 14 is coupled to the audio input device 10, pointing device 11, keyboard 12, display 13, working memory 17 and program memory 15 by bus 18.</p><p num="p0029">The operation of the initialisation section of this embodiment of the present invention will now be described with reference to the flow diagram of Figure 3.</p><p num="p0030">In step S1 the grammar rules are input and in step S2 the word nets are formed for the grammar rules. Words in the word nets to be synchronised with the second modality event or events, are then identified in step S3 and in step S4 finite state machines for the second modality event or events are generated. For each of the words in the word nets a 2-dimensional finite state array for the identified word is then created at step S5 using the word states and the second modality event states. This is illustrated in more detail in Figure 7. Considering the word &quot;these&quot; in the grammar rule, the second modality states (click states) comprise 0, 1, or 2+. The word states comprise six states corresponding to six extracted features of the speech for the word &quot;these&quot;. The transition from each state has associated with it a probability. This probability is used in the recognition process as will be described in more detail hereinafter.</p><p num="p0031">Figure 4 is a flow diagram illustrating the recognition process. In step S6 the speech and second modality input are received and in step S7 the speech features are extracted. In step S8 recognition is performed using the word nets. As can be seen in Figure 5, the grammar rule defines that the words &quot;fax&quot; or &quot;send&quot; should be followed by the words &quot;this&quot;, &quot;that&quot; or &quot;these&quot;. Thus, recognition uses these grammar rules. However, in order to recognise the words &quot;this&quot;, &quot;that&quot; or &quot;these&quot;, recognition is performed using the array of states as illustrated in Figure 7. As features in the speech are input, they are matched to transitions across the array. Also as features i.e. clicks are received in the second modality input, these are matched to transitions down the array. Thus as illustrated in Figure 7, speech features are received which are identified as transitions to word state 3. A click feature is then received and identified as a transition to click state 1. Two further speech features are then received identifying transitions to word state 5. A further click feature is received and identified as a transition to click state 2+ and finally a further speech feature is received and identified as a transition to word state 6. Thus a final state in the word model is reached.</p><p num="p0032">In step S9 in Figure 4, recognised text is output.</p><p num="p0033">As mentioned hereinabove, the state transitions have associated with them probabilities. Thus an accumulated probability is obtained which will be high when the click state 2+ and the word state 6 is reached in the recognition process. A lower probability will be obtained for other states.</p><p num="p0034">The recognition process illustrated in Figure 7 for a single word will be carried out sequentially for sequential words in the word net and where words have second modality events associated therewith, the word models will have a second dimension defined by the second modality states in addition to the first dimension comprising the word states.</p><p num="p0035">The word states used in the recognition process can comprise any conventional states used in speech recognition such as the states of a Hidden Markov model, or the states defined by dynamic time warping.</p><p num="p0036">Although the present invention has been described hereinabove with reference to a specific embodiment, it will be apparent to a skilled person in the art that modifications can be made without departing from the spirit and scope of the present invention.</p><p num="p0037">Although in the embodiment only a single second complementary modality is used, the present invention is applicable to any number of further complementary modalities whereupon the dimensionality of the array of states correspondingly increases.</p><p num="p0038">The second modality input is not restricted to mouse events as given in the embodiment. The second modality input can comprise any other type of pointing device input.</p><p num="p0039">features ot second modality events can be input sequentially to the recognition process e.g. mouse clicks, or simultaneously.</p><p num="p0040">In the above described embodiments second modality inputs or events are associated with single words. The present invention may, however, also be applied where a second modality input or event is associated with a number of words, that is with a phrase.</p><p num="p0041">It can be seen from the description of the embodiment of the present invention and the modifications that the present invention provides accurate recognition in a multimodal input system by using word models in the recognition process which are based not just on speech input but also on the or each further modality input.</p></description><claims lang="DE" status="new"><claim id="c-de-01-0001" num="0001"><claim-text>Verfahren zur Spracherkennung in einem komplementären multimodalen Eingabesystem, mit den Schritten

Empfangen einer komplementären multimodalen Eingabe mit digitalisierter Sprache als Sprachmodalitäteingabe und Zeigeeinrichtungsereignisdaten als Zeigeeinrichtungsmodalitäteingabe,

Identifizieren von Merkmalen in der Sprache und den Zeigeeinrichtungsereignisdaten,
<b>gekennzeichnet durch</b>

Erkennen von Worten aus den identifizierten Merkmalen <b>durch</b> Verwendung von Wortmodellen, wobei jedes Wortmodell ein Dimensionsarray möglicher Zustände umfasst, wobei jede Dimension eine entsprechende Modalitäteingabe darstellt, wobei die Verarbeitung zur Erkennung von Worten unter Verwendung der Wortmodelle ein Zusammenpassen empfangener Merkmale in jeder Modalitäteingabe mit Übergängen zwischen Zuständen in der dieser Modalitäteingabe entsprechenden Dimension zum Erreichen eines Endzustands umfasst.</claim-text></claim><claim id="c-de-01-0002" num="0002"><claim-text>Spracherkennungsverfahren nach Anspruch 1, wobei die Wortmodelle von Worten in einem Wortnetz entsprechend Grammatikregeln organisiert sind.</claim-text></claim><claim id="c-de-01-0003" num="0003"><claim-text>Spracherkennungsverfahren nach einem der vorhergehenden Ansprüche, wobei die Zustände in den Wortmodellen für die Erkennung von Sprache Zustände von Hidden-Markov-Modellen umfassen.</claim-text></claim><claim id="c-de-01-0004" num="0004"><claim-text>Spracherkennungsverfahren nach einem der vorgehenden Ansprüche, wobei die Zustände mit ihnen verbundene Wahrscheinlichkeiten aufweisen und der Erkennungsschritt einen Vergleich der identifizierten Merkmale mit den Zuständen zur Bestimmung eines Worts mit der größten Wahrscheinlichkeit am Endzustand umfasst.</claim-text></claim><claim id="c-de-01-0005" num="0005"><claim-text>Multimodales Eingabeverfahren mit

Verwenden eines Verfahrens nach einem der vorhergehenden Ansprüche zur Erzeugung erkannter Worte,

Verarbeiten der erkannten Worte und der Zeigeeinrichtungsmodalitäteingabe entsprechend Regeln zur Erzeugung einer Eingabe für einen Prozess.</claim-text></claim><claim id="c-de-01-0006" num="0006"><claim-text>Spracherkennungsvorrichtung zur Verwendung in einem komplementären multimodalen Eingabesystem, mit

einer Empfangseinrichtung zum Empfangen einer komplementären multimodalen Eingabe mit digitalisierter Sprache als Sprachmodalitäteingabe und

Zeigeeinrichtungsereignisdaten als Zeigeeinrichtungsmodalitäteingabe,

einer Identifizierungseinrichtung (5, 7a) zum Identifizieren von Merkmalen in der Sprache und in den Zeigeeinrichtungsereignisdaten,
<b>gekennzeichnet durch</b>

eine Erkennungseinrichtung (6) zum Erkennen von Worten aus den identifizierten Merkmalen <b>durch</b> Verwendung von Wortmodellen, wobei jedes Wortmodel ein Dimensionsarray möglicher Zustände umfasst, wobei jede Dimension eine entsprechende Modalitäteingabe darstellt, wobei die Erkennungseinrichtung (6) zum Zusammenpassen empfangener Merkmale in jeder Modalitäteingabe mit Übergängen zwischen Zuständen in der dieser Modalitäteingabe entsprechenden Dimension zum Erreichen eines Endzustands eingerichtet ist.</claim-text></claim><claim id="c-de-01-0007" num="0007"><claim-text>Spracherkennungsvorrichtung nach Anspruch 6, mit einer Speichereinrichtung zur Speicherung der Wortmodelle.</claim-text></claim><claim id="c-de-01-0008" num="0008"><claim-text>Spracherkennungsvorrichtung nach Anspruch 6 oder 7, wobei die Erkennungseinrichtung (6) zur Verwendung der Wortmodelle eingerichtet ist, die in einem Wortnetz entsprechend Grammatikregeln organisiert sind.</claim-text></claim><claim id="c-de-01-0009" num="0009"><claim-text>Spracherkennungsvorrichtung nach einem der Ansprüche 6 bis 8, wobei die Erkennungseinrichtung (6) zur Verwendung von Zuständen von Hidden-Markov-Modellen als Zustände in den Modellen für die Erkennung von Sprache eingerichtet ist.</claim-text></claim><claim id="c-de-01-0010" num="0010"><claim-text>Spracherkennungsvorrichtung nach einem der Ansprüche 6 bis 9, wobei die Zustände der Wortmodelle assoziierte Wahrscheinlichkeiten aufweisen und die Erkennungseinrichtung zum Vergleichen der identifizierten Merkmale mit den Zuständen zur Bestimmung eines Worts mit der größten Wahrscheinlichkeit am Endzustand eingerichtet ist.</claim-text></claim><claim id="c-de-01-0011" num="0011"><claim-text>Multimodales Eingabesystem mit:
<claim-text>einer Spracheingabeeinrichtung (4) zur Eingabe von Sprache,</claim-text><claim-text>einer Sprachdigitalisierungseinrichtung zum Digitalisieren der eingegebenen Sprache zum Bereitstellen digitalisierter Sprache als die Sprachmodalitäteingabe,</claim-text><claim-text>einer Zeigeeinrichtungsmodalitäteingabeeinrichtung. (7) zur Eingabe der Zeigeeinrichtungsereignisdaten,</claim-text><claim-text>einer Spracherkennungsvorrichtung nach einem der Ansprüche 6 bis 10 zur Erzeugung erkannter Worte, und</claim-text><claim-text>einer Verarbeitungseinrichtung zur Verarbeitung der erkannten Worte und der Zeigeeinrichtungsmodalitäteingabe entsprechend Regeln zur Erzeugung einer Eingabe für einen Prozess.</claim-text></claim-text></claim><claim id="c-de-01-0012" num="0012"><claim-text>Verarbeitungssystem zum Implementieren eines Prozesses, mit

dem multimodalen Eingabesystem nach Anspruch 11 zur Erzeugung einer Eingabe und

einer Verarbeitungseinrichtung zur Verarbeitung der erzeugten Eingabe.</claim-text></claim><claim id="c-de-01-0013" num="0013"><claim-text>Programmkode zur Steuerung eines Prozessors zum Implementieren des Verfahrens nach einem der Ansprüche 1 bis 5.</claim-text></claim><claim id="c-de-01-0014" num="0014"><claim-text>Trägermedium, das den Programmkode nach Anspruch 13 trägt.</claim-text></claim></claims><claims lang="EN" status="new"><claim id="c-en-01-0001" num="0001"><claim-text>A method of recognising speech in a complementary multimodal input system, the method comprising the steps of:
<claim-text>receiving a complementary multimodal input comprising digitized speech as a speech modality input and pointing device event data as a pointing device modality input;</claim-text><claim-text>identifying features in the speech and the pointing device event data; and
<b>characterised by</b></claim-text><claim-text>recognising words from the identified features by using word models with each word model comprising a dimensional array of possible states with each dimension representing a corresponding one of the modality inputs, wherein the processing to recognise words using said word models comprises matching received features in each modality input to transitions between states in the dimension corresponding to that modality input in an attempt to reach a final state.</claim-text></claim-text></claim><claim id="c-en-01-0002" num="0002"><claim-text>A speech recognition method according to claim 1, wherein said word models of words are organised in a word net in accordance with grammar rules.</claim-text></claim><claim id="c-en-01-0003" num="0003"><claim-text>A speech recognition method according to any preceding claim, wherein the states in the word models for the recognition of speech comprise states of Hidden Markov models.</claim-text></claim><claim id="c-en-01-0004" num="0004"><claim-text>A speech recognition method according to any preceding claim, wherein said states have probabilities associated therewith and the recognition step comprises comparing the identified features with the states to determine a word with the highest probability at the final state.</claim-text></claim><claim id="c-en-01-0005" num="0005"><claim-text>A multimodal input method comprising:
<claim-text>using a method according to any preceding claim to generate recognised words;</claim-text><claim-text>processing the recognised words and the pointing device modality input in accordance with rules to generate an input for a process.</claim-text></claim-text></claim><claim id="c-en-01-0006" num="0006"><claim-text>Speech recognition apparatus for use in a complementary multimodal input system, the apparatus comprising:
<claim-text>receiving means for receiving a complementary multimodal input comprising digitized speech as a speech modality input and pointing device event data as a pointing device modality input;</claim-text><claim-text>identifying means (5, 7a) for identifying features in the speech and in the pointing device event data; and
<b>characterised by</b></claim-text><claim-text>recognising means (6) for recognising words from the identified features by using word models with each word model comprising a dimensional array of possible states with each dimension representing a corresponding one of the modality inputs, the recognising means (6) being operable to match received features in each modality input to transitions between states in the dimension corresponding to that modality input in an attempt to reach a final state.</claim-text></claim-text></claim><claim id="c-en-01-0007" num="0007"><claim-text>Speech recognition apparatus according to claim 6, including storage means storing said word models.</claim-text></claim><claim id="c-en-01-0008" num="0008"><claim-text>Speech recognition apparatus according to claim 6 or 7, wherein said recognition means (6) is adapted to use said word models organised in a word net in accordance with grammar rules.</claim-text></claim><claim id="c-en-01-0009" num="0009"><claim-text>Speech recognition apparatus according to any of claims 6 to 8, wherein said recognition means (6) is adapted to use states of Hidden Markov models as the states in the models for the recognition of speech.</claim-text></claim><claim id="c-en-01-0010" num="0010"><claim-text>Speech recognition apparatus according to any of claims 6 to 9, wherein the states of the word models have associated probabilities and the recognising means is operable to compare the identified features with the states to determine a word with the highest probability at the final state.</claim-text></claim><claim id="c-en-01-0011" num="0011"><claim-text>A multimodal input system comprising:
<claim-text>speech input means (4) for inputting speech;</claim-text><claim-text>speech digitizing means for digitizing the input speech to provide digitized speech as the speech modality input;</claim-text><claim-text>pointing device modality input means (7) for inputting the pointing device event data;</claim-text><claim-text>speech recognition apparatus according to any of claims 6 to 10 for generating recognised words; and</claim-text><claim-text>processing means for processing the recognized words and the pointing device modality input in accordance with rules to generate an input for a process.</claim-text></claim-text></claim><claim id="c-en-01-0012" num="0012"><claim-text>A processing system for implementing a process, the system comprising:
<claim-text>the multimodal input system according to claim 11 for generating an input; and</claim-text><claim-text>processing means for processing the generated input.</claim-text></claim-text></claim><claim id="c-en-01-0013" num="0013"><claim-text>Program code for controlling a processor to implement the method of any of claims 1 to 5.</claim-text></claim><claim id="c-en-01-0014" num="0014"><claim-text>A carrier medium carrying the program code according to claim 13.</claim-text></claim></claims><claims lang="FR" status="new"><claim id="c-fr-01-0001" num="0001"><claim-text>Procédé de reconnaissance vocale dans un système de données d'entrée multimodales complémentaires, ce procédé comprenant les étapes consistant à:
<claim-text>recevoir une donnée d'entrée multimodale complémentaire comprenant des paroles numérisées en tant que donnée d'entrée de modalité vocale et des données d'événement de dispositif de pointage en tant que donnée d'entrée de modalité de dispositif de pointage ;</claim-text><claim-text>identifier des caractéristiques dans la parole et dans les données d'événement de dispositif de pointage ; et
<b>caractérisé par</b></claim-text><claim-text>la reconnaissance de mots à partir de caractéristiques identifiées à l'aide de modèles de mots, chaque modèle de mot comprenant une matrice dimensionnelle des états possibles, chaque dimension représentant une dimension correspondante des données d'entrée de modalité, dans lequel le traitement de reconnaissance de mots à l'aide desdits modèles de mots comporte la mise en correspondance de caractéristiques reçues dans chaque donnée d'entrée de modalité avec des transitions entre états dans la dimension correspondante à cette donnée d'entrée de modalité afin d'atteindre un état final.</claim-text></claim-text></claim><claim id="c-fr-01-0002" num="0002"><claim-text>Procédé de reconnaissance vocale selon la revendication 1 dans lequel lesdits modèles de mots sont organisés dans un réseau de mots conformément aux règles grammaticales.</claim-text></claim><claim id="c-fr-01-0003" num="0003"><claim-text>Procédé de reconnaissance vocale selon l'une quelconque des revendications précédentes, dans lequel les états dans les modèles de mots pour la reconnaissance vocale comprennent des états de modèles de Markov cachés.</claim-text></claim><claim id="c-fr-01-0004" num="0004"><claim-text>Procédé de reconnaissance vocale selon l'une quelconque des revendications précédentes, dans lequel lesdits états ont des probabilités qui leur sont associées et la phase de reconnaissance comprend la comparaison des caractéristiques identifiées avec les états pour déterminer un mot avec la plus grande probabilité à l'état final.</claim-text></claim><claim id="c-fr-01-0005" num="0005"><claim-text>Procédé d'entrées multimodales comprenant :
<claim-text>l'utilisation d'un procédé selon l'une quelconque des revendications précédentes pour générer des mots reconnus ;</claim-text><claim-text>le traitement des mots reconnus et des données d'entrée de modalité de dispositif de pointage conformément aux règles pour générer une donnée d'entrée pour un traitement.</claim-text></claim-text></claim><claim id="c-fr-01-0006" num="0006"><claim-text>Appareil de reconnaissance vocale à utiliser dans un système de données d'entrée multimodales complémentaires, l'appareil comprenant :
<claim-text>des moyens de réception pour recevoir une donnée d'entrée multimodale complémentaire comprenant des paroles numérisées en tant que donnée d'entrée de modalité vocale et des données d'événement de dispositif de pointage en tant que donnée d'entrée de modalité de dispositif de pointage ;</claim-text><claim-text>des moyens d'identification (5, 7a) pour identifier des caractéristiques dans la parole et dans les données d'événement de dispositif de pointage ; et
<b>caractérisé par</b>

de moyens de reconnaissance (6) pour reconnaître des mots à partir de caractéristiques identifiées à l'aide de modèles de mots, chaque modèle de mot comprenant une matrice dimensionnelle des états possibles, chaque dimension représentant une dimension correspondante des données d'entrée de modalité, les moyens de reconnaissance (6) pouvant fonctionner pour faire correspondre des caractéristiques reçues dans chaque donnée d'entrée de modalité à des transitions entre des états dans la dimension correspondant à cette donnée d'entrée de modalité afin d'atteindre un état final.</claim-text></claim-text></claim><claim id="c-fr-01-0007" num="0007"><claim-text>Appareil de reconnaissance vocale selon la revendication 6, comprenant des moyens de stockage contenant lesdits modèles de mots.</claim-text></claim><claim id="c-fr-01-0008" num="0008"><claim-text>Appareil de reconnaissance vocale selon la revendication 6 ou 7 dans lequel lesdits moyens de reconnaissance (6) sont adaptés à l'utilisation desdits modèles de mots organisés dans un réseau de mots conformément aux règles grammaticales.</claim-text></claim><claim id="c-fr-01-0009" num="0009"><claim-text>Appareil de reconnaissance vocale selon l'une quelconque des revendications 6 à 8 dans lequel lesdits moyens de reconnaissance (6) sont adaptés à l'utilisation des états des modèles de Markov cachés en tant qu'états dans les modèles pour la reconnaissance vocale.</claim-text></claim><claim id="c-fr-01-0010" num="0010"><claim-text>Appareil de reconnaissance vocale selon l'une quelconque des revendications 6 à 9 dans lequel les états des modèles de mots ont des probabilités associées et les moyens de reconnaissance peuvent fonctionner pour comparer les caractéristiques identifiées aux états pour déterminer un mot avec la plus grande probabilité à l'état final.</claim-text></claim><claim id="c-fr-01-0011" num="0011"><claim-text>Système de données d'entrée multimodales comprenant :
<claim-text>des moyens de données d'entrée vocales (4) pour entrer des paroles ;</claim-text><claim-text>des moyens de numérisation de la parole pour numériser la parole entrée afin de fournir la parole numérisée comme donnée d'entrée de modalité vocale ;</claim-text><claim-text>des moyens de données d'entrée de modalité de dispositif de pointage (7) pour entrer les données d'entrée d'événement de dispositif de pointage ;</claim-text><claim-text>un appareil de reconnaissance vocale selon l'une quelconque des revendications 6 à 10 pour générer des mots reconnus ; et</claim-text><claim-text>des moyens de traitement pour traiter les mots reconnus et la donnée d'entrée de modalité de dispositif de pointage conformément aux règles pour générer une donnée d'entrée pour un traitement.</claim-text></claim-text></claim><claim id="c-fr-01-0012" num="0012"><claim-text>Système de traitement pour mettre en place un traitement, le système comprenant :<claim-text>le système de données d'entrée multimodales selon la revendication 11 pour générer une donnée d'entrée ; et</claim-text><claim-text>les moyens de traitement pour traiter la donnée d'entrée générée.</claim-text></claim-text></claim><claim id="c-fr-01-0013" num="0013"><claim-text>Code de programme pour contrôler un processeur afin de mettre en place le procédé selon l'une quelconque des revendications 1 à 5.</claim-text></claim><claim id="c-fr-01-0014" num="0014"><claim-text>Moyen de support supportant le code de programme selon la revendication 13.</claim-text></claim></claims><legal-status status="new"><legal-event country="EP" code="ET" date="20070105"><legal-event-body><event-title>FR: TRANSLATION FILED</event-title></legal-event-body></legal-event><legal-event country="EP" code="REF" date="20060720"><legal-event-body><event-title>CORRESPONDS TO:</event-title><event-attributes><event-attribute><event-attribute-label>Ref Document Number </event-attribute-label><event-attribute-value>60120247</event-attribute-value></event-attribute><event-attribute><event-attribute-label>Country of Ref Document </event-attribute-label><event-attribute-value>DE</event-attribute-value></event-attribute><event-attribute><event-attribute-label>Date of Ref Document </event-attribute-label><event-attribute-value>20060720</event-attribute-value></event-attribute><event-attribute><event-attribute-label>Kind Code of Ref Document </event-attribute-label><event-attribute-value>P</event-attribute-value></event-attribute></event-attributes></legal-event-body></legal-event><legal-event country="EP" code="AK" date="20060607"><legal-event-body><event-title>DESIGNATED CONTRACTING STATES:</event-title><event-attributes><event-attribute><event-attribute-label>Kind Code of Ref Document </event-attribute-label><event-attribute-value>B1</event-attribute-value></event-attribute><event-attribute><event-attribute-label>Designated State(s) </event-attribute-label><event-attribute-value>DEFRGB</event-attribute-value></event-attribute></event-attributes></legal-event-body></legal-event><legal-event country="EP" code="REG" date="20060607"><legal-event-body><event-title>REFERENCE TO A NATIONAL CODE</event-title><event-attributes><event-attribute><event-attribute-label>Ref Country Code </event-attribute-label><event-attribute-value>GB</event-attribute-value></event-attribute><event-attribute><event-attribute-label>Ref Legal Event Code </event-attribute-label><event-attribute-value>FG4D</event-attribute-value></event-attribute></event-attributes></legal-event-body></legal-event><legal-event country="EP" code="17Q" date="20050330"><legal-event-body><event-title>FIRST EXAMINATION REPORT</event-title><event-attributes><event-attribute><event-attribute-label>Effective Date </event-attribute-label><event-attribute-value>20050211</event-attribute-value></event-attribute></event-attributes></legal-event-body></legal-event><legal-event country="EP" code="AKX" date="20020612"><legal-event-body><event-title>PAYMENT OF DESIGNATION FEES</event-title><event-attributes><event-attribute><event-attribute-label>free format text </event-attribute-label><event-attribute-value>DE FR GB</event-attribute-value></event-attribute></event-attributes></legal-event-body></legal-event><legal-event country="EP" code="17P" date="20020424"><legal-event-body><event-title>REQUEST FOR EXAMINATION FILED</event-title><event-attributes><event-attribute><event-attribute-label>Effective Date </event-attribute-label><event-attribute-value>20020218</event-attribute-value></event-attribute></event-attributes></legal-event-body></legal-event><legal-event country="EP" code="AK" date="20010926"><legal-event-body><event-title>DESIGNATED CONTRACTING STATES:</event-title><event-attributes><event-attribute><event-attribute-label>Kind Code of Ref Document </event-attribute-label><event-attribute-value>A3</event-attribute-value></event-attribute><event-attribute><event-attribute-label>Designated State(s) </event-attribute-label><event-attribute-value>ATBECHCYDEDKESFIFRGBGRIEITLILUMCNLPTSETR</event-attribute-value></event-attribute></event-attributes></legal-event-body></legal-event><legal-event country="EP" code="AX" date="20010926"><legal-event-body><event-title>EXTENSION OF THE EUROPEAN PATENT TO</event-title><event-attributes><event-attribute><event-attribute-label>free format text </event-attribute-label><event-attribute-value>AL;LT;LV;MK;RO;SI</event-attribute-value></event-attribute></event-attributes></legal-event-body></legal-event><legal-event country="EP" code="RIC1" date="20010926"><legal-event-body><event-title>CLASSIFICATION (CORRECTION)</event-title><event-attributes><event-attribute><event-attribute-label>free format text </event-attribute-label><event-attribute-value>7G 10L 15/24 A, 7G 10L 15/18 B, 7G 10L 15/26 B, 7G 06F 3/033 B, 7G 06F 3/16 B, 7G 06F 17/27 B</event-attribute-value></event-attribute></event-attributes></legal-event-body></legal-event><legal-event country="EP" code="AK" date="20010822"><legal-event-body><event-title>DESIGNATED CONTRACTING STATES:</event-title><event-attributes><event-attribute><event-attribute-label>Kind Code of Ref Document </event-attribute-label><event-attribute-value>A2</event-attribute-value></event-attribute><event-attribute><event-attribute-label>Kind Code of Ref Document </event-attribute-label><event-attribute-value>A2</event-attribute-value></event-attribute><event-attribute><event-attribute-label>Designated State(s) </event-attribute-label><event-attribute-value>ATBECHCYDEDKESFIFRGBGRIEITLILUMCNLPTSETR</event-attribute-value></event-attribute><event-attribute><event-attribute-label>Designated State(s) </event-attribute-label><event-attribute-value>ATBECHCYDEDKESFIFRGBGRIEITLILUMCNLPTSETR</event-attribute-value></event-attribute></event-attributes></legal-event-body></legal-event><legal-event country="EP" code="AX" date="20010822"><legal-event-body><event-title>EXTENSION OF THE EUROPEAN PATENT TO</event-title><event-attributes><event-attribute><event-attribute-label>free format text </event-attribute-label><event-attribute-value>AL;LT;LV;MK;RO;SI</event-attribute-value></event-attribute></event-attributes></legal-event-body></legal-event></legal-status></patent-document>